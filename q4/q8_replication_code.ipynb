{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas and numpy necessary to do basic data cleaning\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Data and Generate Dummy Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_aer_data(df):\n",
    "\n",
    "    # Clean up data types\n",
    "    df['year'] = df.year.dt.year\n",
    "    for x in ['year','fips','stfips']:\n",
    "        df[x] = df[x].astype(int)\n",
    "\n",
    "    # Filter data the same as paper authors\n",
    "    df = df[df.year <= 1988]\n",
    "    df = df[~((df.stfips == 36) & (df.cofips == 61))] \n",
    "    df = df[~((df.stfips == 6) & (df.cofips == 37))]\n",
    "    df = df[~((df.stfips == 17) & (df.cofips == 31))]\n",
    "\n",
    "    # Make new variables related to urbanization\n",
    "    df_1960_pcturban = df.loc[df.year == 1960, ['fips','D_60pcturban_t']] \\\n",
    "                         .drop_duplicates('fips') \\\n",
    "                         .rename(columns={'D_60pcturban_t':'_urb'}) \\\n",
    "                         .fillna(0)\n",
    "    df = df.merge(df_1960_pcturban, how='left', on='fips')\n",
    "    df['Durb'] = pd.cut(x=df._urb, bins=[0,1,25,50,75,110], right=False, labels=[0,1,25,50,75])\n",
    "\n",
    "    # Make straight-up dummy variables\n",
    "    year_dummies   = pd.get_dummies(df.year, prefix=\"_Iyear\", drop_first=True)\n",
    "    Durb_dummies   = pd.get_dummies(df.Durb, prefix=\"_IDurb\", drop_first=True)\n",
    "    fips_dummies   = pd.get_dummies(df.fips, prefix=\"_Ifips\", drop_first=True)\n",
    "    stfips_dummies = pd.get_dummies(df.stfips, prefix=\"_Istfips\", drop_first=True)\n",
    "\n",
    "    # Make interaction dummies\n",
    "    for year in np.sort(df.year.unique()):\n",
    "        for Durb in np.sort(df.Durb.unique()):\n",
    "            if (f\"_Iyear_{year}\" in year_dummies.columns) & (f\"_IDurb_{Durb}\" in Durb_dummies.columns):\n",
    "                df[f\"_IyeaXDur_{year}_{Durb}\"] = year_dummies[f\"_Iyear_{year}\"]*Durb_dummies[f\"_IDurb_{Durb}\"]\n",
    "        for stfips in np.sort(df.stfips.unique()):\n",
    "            if (f\"_Iyear_{year}\" in year_dummies.columns) & (f\"_Istfips_{stfips}\" in stfips_dummies.columns):\n",
    "                df[f\"_IyeaXstf_{year}_{stfips}\"] = year_dummies[f\"_Iyear_{year}\"]*stfips_dummies[f\"_Istfips_{stfips}\"]\n",
    "    for fips in np.sort(df.fips.unique()):\n",
    "        df[f\"_IfipXyea_{fips}\"] = np.where((df['fips']) == fips, df['year'], 0)\n",
    "\n",
    "    # Make did1 dummies\n",
    "    for i,did1 in enumerate(np.sort(df.did1.unique())):\n",
    "        if did1 != -1:\n",
    "            df[f\"_DDdid1_{i+1}\"] = np.where(df['did1'] == did1, 1, 0)\n",
    "\n",
    "    # Add on year and fips dummies\n",
    "    df = pd.concat([df,year_dummies],axis=1)\n",
    "    df = pd.concat([df,fips_dummies],axis=1)\n",
    "\n",
    "    # Drop missing response values\n",
    "    df = df[~df['amr'].isna()]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and clean data\n",
    "df = pd.read_stata(\"./aer_data/aer_data.dta\")\n",
    "df = clean_aer_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Regression Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_regression_matrices(df):\n",
    "\n",
    "    # Define response vector\n",
    "    y = df['amr'].to_numpy()[:,None]\n",
    "\n",
    "    # Define covariate matrix\n",
    "    df['_const']  = 1\n",
    "    _Ifips_cols   = [col for col in df if col.startswith('_Ifips')]\n",
    "    _Iyear_cols   = [col for col in df if col.startswith('_Iyear')]\n",
    "    _IyeaXDu_cols = [col for col in df if col.startswith('_IyeaXDu')]\n",
    "    _DD_cols      = [col for col in df if col.startswith('_DD')]\n",
    "    X_cols        = _Ifips_cols+_Iyear_cols+_IyeaXDu_cols+_DD_cols+['_const'] \n",
    "    X             = df[X_cols].to_numpy()\n",
    "\n",
    "    # Define weighting vector\n",
    "    w_popwt       = df['popwt'].to_numpy()[:,None]\n",
    "\n",
    "    return y,X,X_cols,w_popwt,df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y,X,X_cols,w_popwt,df = make_regression_matrices(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Coefficient and SE Estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fe_weighted_regression(y,X,w_popwt,df):\n",
    "    \n",
    "    # Get beta value\n",
    "    beta = np.linalg.pinv(np.multiply(X,w_popwt).T @ X) @ np.multiply(X,w_popwt).T @ y\n",
    "\n",
    "    # Build the 'meat' of the cluster sandwich SE estimator\n",
    "    clust_cov_sum = np.zeros((len(beta),len(beta)))\n",
    "    for clust in np.sort(df.fips.unique()):\n",
    "\n",
    "        # Define data just from cluster\n",
    "        df_clust = df[df.fips == clust]\n",
    "        X_clust  = df_clust[X_cols].to_numpy()\n",
    "        y_clust  = df_clust['amr'].to_numpy()[:,None]\n",
    "        w_popwt_clust = df_clust['popwt'].to_numpy()[:,None]\n",
    "\n",
    "        # Do weighted cluster robust SE formula\n",
    "        u_j  = np.multiply((y_clust - X_clust @ beta), X_clust)\n",
    "        wu_j = np.multiply(w_popwt_clust, u_j)\n",
    "        clust_sum = np.sum(wu_j, axis=0)[None,:]\n",
    "        clust_cov = clust_sum.T @ clust_sum\n",
    "\n",
    "        # Add to overall \n",
    "        clust_cov_sum += clust_cov\n",
    "\n",
    "    # Get (X'X)^(-1): the 'bread' of the sandwich\n",
    "    vcov = np.linalg.pinv(np.multiply(X,w_popwt).T @ X)\n",
    "    vcov = np.where(vcov < 0, 0, vcov)\n",
    "\n",
    "    # Finite-sample correction\n",
    "    n_clust = df.fips.unique().shape[0]\n",
    "    N       = np.sum(df['popwt'])\n",
    "    k       = beta.shape[0]\n",
    "    qc      = (n_clust/(n_clust-1)) * (N/(N-k))\n",
    "\n",
    "    # Get standard errors of betas\n",
    "    SE = np.sqrt(np.diag(qc * vcov @ clust_cov_sum @ vcov))\n",
    "    \n",
    "    return beta,vcov,SE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta,vcov,SE = run_fe_weighted_regression(y,X,w_popwt,df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Results in Table and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        beta_name  beta_values    beta_se\n",
      "3061  _Iyear_1960    12.672612  26.253466\n",
      "3062  _Iyear_1961   -15.191984  25.961281\n",
      "3063  _Iyear_1962     7.382851  27.877679\n",
      "3064  _Iyear_1963    25.784916  31.001602\n",
      "3065  _Iyear_1964     2.305644  38.843589\n",
      "3066  _Iyear_1965     0.061686  35.270078\n",
      "3067  _Iyear_1966    -0.821723  36.798754\n",
      "3068  _Iyear_1967   -16.789440  37.756279\n",
      "3069  _Iyear_1968     1.279656  41.673243\n",
      "3070  _Iyear_1969    -2.463052  42.303299\n",
      "3071  _Iyear_1970   -26.824601  40.544035\n",
      "3072  _Iyear_1971   -50.165184  43.566103\n",
      "3073  _Iyear_1972   -47.168861  44.770084\n",
      "3074  _Iyear_1973   -56.113462  48.548970\n",
      "3075  _Iyear_1974  -105.138011  45.283146\n",
      "              beta_name  beta_values    beta_se\n",
      "3198   _IyeaXDur_1987_1   -12.613402  57.973286\n",
      "3199  _IyeaXDur_1987_25   -11.973542  57.578462\n",
      "3200  _IyeaXDur_1987_50   -16.232936  58.899123\n",
      "3201  _IyeaXDur_1987_75   -18.219819  62.532406\n",
      "3202   _IyeaXDur_1988_1   -12.366518  59.497630\n",
      "3203  _IyeaXDur_1988_25   -14.603190  59.123421\n",
      "3204  _IyeaXDur_1988_50   -20.891239  60.542477\n",
      "3205  _IyeaXDur_1988_75   -20.716013  64.231034\n",
      "3206          _DDdid1_1     3.872314   4.971656\n",
      "3207          _DDdid1_2     0.033880   3.135779\n",
      "3208          _DDdid1_4    -5.636439   3.758693\n",
      "3209          _DDdid1_5   -12.045204   5.261462\n",
      "3210          _DDdid1_6    -9.384156   6.971082\n",
      "3211          _DDdid1_7    -9.166914  10.232743\n",
      "3212             _const  1019.556930   0.024384\n"
     ]
    }
   ],
   "source": [
    "beta_table = pd.DataFrame.from_dict({\"beta_name\":X_cols,\n",
    "                                     \"beta_values\":beta.ravel(),\n",
    "                                     \"beta_se\":SE})\n",
    "beta_table = beta_table[~beta_table.beta_name.str.contains('^_Ifips_')]\n",
    "print(beta_table.head(15))\n",
    "print(beta_table.tail(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export\n",
    "beta_table = beta_table[beta_table.beta_name.str.contains('^_DDdid1_')]\n",
    "beta_table.to_csv('table2_replication.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
